{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDRi1j9iLie3YusXIPbxIF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllCoolNicknamesWereTaken/Gany/blob/master/dcgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRQvjBFah-FT"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOhjwQiS3Wnd",
        "outputId": "deced4e6-89cd-4a99-c21d-417ab9f968d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f21qvyZL3XPK"
      },
      "source": [
        "import glob\n",
        "def load_paths(dataset):\n",
        "#ZAMIST 2 FORA MAPOWANIE NWM JAKZROBIC LAMBDA FUNCTION?\n",
        "  paths = []\n",
        "  for name in glob.glob('./drive/My Drive/danedopracy/datasets/%s/*' % dataset):\n",
        "    for i in glob.glob('%s/*' % name ): \n",
        "      paths.append(i) \n",
        "  return paths\n",
        "\n",
        "load_paths('shapes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otq772oFFQbe",
        "outputId": "1f57c82e-b5db-4721-baf1-80e238be19c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install pillow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECVnoYi-_nnD"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "import imageio\n",
        "from skimage import transform\n",
        "\n",
        "def load_images(dataset):\n",
        "  paths = load_paths('shapes')\n",
        "  train_paths = list(np.random.choice(paths, int(0.7 * len(paths))))\n",
        "\n",
        "  # test = []\n",
        "  train = []\n",
        "  for train_img in train_paths:\n",
        "    # test.append(np.array(imageio.imread(test_img)))\n",
        "    res_img = transform.resize(imageio.imread(train_img), (28,28), mode='symmetric', preserve_range=True)\n",
        "    train.append(np.array(res_img))\n",
        " \n",
        "  return np.array(train)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFRts7yxEN-q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8PWN9GrLvzz",
        "outputId": "c728e0c7-7d69-4a3c-b4f7-6da6fb23fc77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DCGAN():\n",
        "    def __init__(self, dataset_name):\n",
        "        # Input shape\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        self.dataset_name = dataset_name\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "        self.generator = self.build_generator()\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        valid = self.discriminator(img)\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((7, 7, 128)))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "        X_train = load_images(self.dataset_name)\n",
        "        print(X_train)\n",
        "\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "#dyskryminator\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            # Sample noise and generate a batch of new images\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator (real classified as ones and generated as zeros)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "#generator\n",
        "\n",
        "\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % save_interval == 0:\n",
        "                self.save_imgs(epoch)\n",
        "\n",
        "    def save_imgs(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"/content/drive/My Drive/danedopracy/datasets/images/%s/%s_%d.png\" % (self.dataset_name, self.dataset_name, epoch))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dcgan = DCGAN('shapes')\n",
        "    dcgan.train(epochs=10000, batch_size=62, save_interval=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_62 (Conv2D)           (None, 14, 14, 32)        320       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_63 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_9 (ZeroPaddin (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_64 (Conv2D)           (None, 8, 8, 256)         147712    \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_38 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 16385     \n",
            "=================================================================\n",
            "Total params: 184,193\n",
            "Trainable params: 183,553\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_19 (Dense)             (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "reshape_9 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_18 (UpSampling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_65 (Conv2D)           (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_19 (UpSampling (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_66 (Conv2D)           (None, 28, 28, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_67 (Conv2D)           (None, 28, 28, 1)         577       \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 856,193\n",
            "Trainable params: 855,809\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "[[[255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  ...\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]]\n",
            "\n",
            " [[255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  ...\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]]\n",
            "\n",
            " [[255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  ...\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  ...\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]]\n",
            "\n",
            " [[255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  ...\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]]\n",
            "\n",
            " [[255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  ...\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]\n",
            "  [255. 255. 255. ... 255. 255. 255.]]]\n",
            "0 [D loss: 0.928597, acc.: 41.94%] [G loss: 0.712088]\n",
            "1 [D loss: 0.404427, acc.: 76.61%] [G loss: 0.763448]\n",
            "2 [D loss: 0.453532, acc.: 75.00%] [G loss: 0.919944]\n",
            "3 [D loss: 0.258348, acc.: 91.94%] [G loss: 1.096985]\n",
            "4 [D loss: 0.291127, acc.: 90.32%] [G loss: 1.361883]\n",
            "5 [D loss: 0.352746, acc.: 86.29%] [G loss: 1.638613]\n",
            "6 [D loss: 0.306881, acc.: 90.32%] [G loss: 1.933425]\n",
            "7 [D loss: 0.226587, acc.: 95.97%] [G loss: 1.966728]\n",
            "8 [D loss: 0.156619, acc.: 98.39%] [G loss: 1.998402]\n",
            "9 [D loss: 0.214213, acc.: 91.13%] [G loss: 2.074564]\n",
            "10 [D loss: 0.187914, acc.: 95.97%] [G loss: 2.212833]\n",
            "11 [D loss: 0.260963, acc.: 90.32%] [G loss: 2.242654]\n",
            "12 [D loss: 0.301348, acc.: 87.10%] [G loss: 2.293904]\n",
            "13 [D loss: 0.387065, acc.: 83.87%] [G loss: 2.277542]\n",
            "14 [D loss: 0.539043, acc.: 72.58%] [G loss: 2.273384]\n",
            "15 [D loss: 0.624887, acc.: 71.77%] [G loss: 2.240174]\n",
            "16 [D loss: 0.803441, acc.: 57.26%] [G loss: 2.024591]\n",
            "17 [D loss: 0.642242, acc.: 66.13%] [G loss: 2.603272]\n",
            "18 [D loss: 1.081682, acc.: 34.68%] [G loss: 2.070915]\n",
            "19 [D loss: 0.793840, acc.: 50.81%] [G loss: 1.832046]\n",
            "20 [D loss: 0.701105, acc.: 59.68%] [G loss: 1.595877]\n",
            "21 [D loss: 1.036398, acc.: 37.90%] [G loss: 1.458940]\n",
            "22 [D loss: 0.696805, acc.: 55.65%] [G loss: 1.377652]\n",
            "23 [D loss: 0.998687, acc.: 41.94%] [G loss: 1.644873]\n",
            "24 [D loss: 1.155281, acc.: 34.68%] [G loss: 1.960611]\n",
            "25 [D loss: 1.176483, acc.: 32.26%] [G loss: 1.730711]\n",
            "26 [D loss: 1.277431, acc.: 31.45%] [G loss: 1.836437]\n",
            "27 [D loss: 1.059711, acc.: 41.13%] [G loss: 1.656623]\n",
            "28 [D loss: 0.944032, acc.: 44.35%] [G loss: 1.787621]\n",
            "29 [D loss: 1.004814, acc.: 36.29%] [G loss: 1.260849]\n",
            "30 [D loss: 0.940374, acc.: 41.13%] [G loss: 1.198625]\n",
            "31 [D loss: 1.063170, acc.: 43.55%] [G loss: 1.447634]\n",
            "32 [D loss: 1.227837, acc.: 27.42%] [G loss: 1.230776]\n",
            "33 [D loss: 1.097729, acc.: 37.90%] [G loss: 1.525774]\n",
            "34 [D loss: 1.135065, acc.: 31.45%] [G loss: 1.739133]\n",
            "35 [D loss: 1.062786, acc.: 32.26%] [G loss: 1.303430]\n",
            "36 [D loss: 1.229565, acc.: 25.81%] [G loss: 1.257826]\n",
            "37 [D loss: 1.066817, acc.: 40.32%] [G loss: 1.456315]\n",
            "38 [D loss: 0.926614, acc.: 42.74%] [G loss: 1.473500]\n",
            "39 [D loss: 1.153264, acc.: 33.06%] [G loss: 1.313722]\n",
            "40 [D loss: 0.906794, acc.: 43.55%] [G loss: 1.529980]\n",
            "41 [D loss: 1.062515, acc.: 34.68%] [G loss: 1.341620]\n",
            "42 [D loss: 0.932504, acc.: 44.35%] [G loss: 1.204219]\n",
            "43 [D loss: 0.933094, acc.: 44.35%] [G loss: 1.312065]\n",
            "44 [D loss: 0.883580, acc.: 45.97%] [G loss: 1.105437]\n",
            "45 [D loss: 0.801026, acc.: 54.03%] [G loss: 1.076957]\n",
            "46 [D loss: 0.993833, acc.: 41.94%] [G loss: 1.300151]\n",
            "47 [D loss: 1.058291, acc.: 30.65%] [G loss: 1.458986]\n",
            "48 [D loss: 1.150587, acc.: 26.61%] [G loss: 1.284328]\n",
            "49 [D loss: 1.048292, acc.: 34.68%] [G loss: 1.438476]\n",
            "50 [D loss: 1.003795, acc.: 37.90%] [G loss: 1.318368]\n",
            "51 [D loss: 0.927559, acc.: 44.35%] [G loss: 1.267215]\n",
            "52 [D loss: 1.051257, acc.: 39.52%] [G loss: 1.214997]\n",
            "53 [D loss: 0.962694, acc.: 44.35%] [G loss: 1.235975]\n",
            "54 [D loss: 0.831053, acc.: 50.00%] [G loss: 1.231809]\n",
            "55 [D loss: 0.852358, acc.: 47.58%] [G loss: 1.277424]\n",
            "56 [D loss: 0.897283, acc.: 45.97%] [G loss: 1.222235]\n",
            "57 [D loss: 0.933678, acc.: 38.71%] [G loss: 1.261290]\n",
            "58 [D loss: 0.932662, acc.: 43.55%] [G loss: 1.224213]\n",
            "59 [D loss: 1.076455, acc.: 30.65%] [G loss: 1.181186]\n",
            "60 [D loss: 0.937427, acc.: 41.13%] [G loss: 1.292058]\n",
            "61 [D loss: 1.010287, acc.: 33.87%] [G loss: 1.040387]\n",
            "62 [D loss: 0.955743, acc.: 41.13%] [G loss: 1.281680]\n",
            "63 [D loss: 0.978611, acc.: 41.94%] [G loss: 1.170080]\n",
            "64 [D loss: 0.888637, acc.: 46.77%] [G loss: 1.197959]\n",
            "65 [D loss: 0.945831, acc.: 37.10%] [G loss: 1.004511]\n",
            "66 [D loss: 0.808197, acc.: 56.45%] [G loss: 1.078534]\n",
            "67 [D loss: 0.898247, acc.: 41.13%] [G loss: 1.204679]\n",
            "68 [D loss: 0.919416, acc.: 44.35%] [G loss: 1.145012]\n",
            "69 [D loss: 0.867428, acc.: 43.55%] [G loss: 1.148323]\n",
            "70 [D loss: 0.885996, acc.: 45.16%] [G loss: 1.229003]\n",
            "71 [D loss: 0.880835, acc.: 42.74%] [G loss: 1.175459]\n",
            "72 [D loss: 1.174879, acc.: 27.42%] [G loss: 1.097850]\n",
            "73 [D loss: 0.974239, acc.: 37.10%] [G loss: 1.224314]\n",
            "74 [D loss: 0.900367, acc.: 41.94%] [G loss: 1.127601]\n",
            "75 [D loss: 1.013551, acc.: 36.29%] [G loss: 1.037414]\n",
            "76 [D loss: 0.922330, acc.: 37.90%] [G loss: 1.002360]\n",
            "77 [D loss: 0.815742, acc.: 48.39%] [G loss: 1.055750]\n",
            "78 [D loss: 1.174652, acc.: 18.55%] [G loss: 1.097363]\n",
            "79 [D loss: 0.792127, acc.: 52.42%] [G loss: 1.197398]\n",
            "80 [D loss: 0.871934, acc.: 43.55%] [G loss: 1.182664]\n",
            "81 [D loss: 0.971752, acc.: 33.87%] [G loss: 1.144153]\n",
            "82 [D loss: 0.807069, acc.: 50.81%] [G loss: 1.340334]\n",
            "83 [D loss: 1.076505, acc.: 31.45%] [G loss: 1.315309]\n",
            "84 [D loss: 0.928269, acc.: 45.97%] [G loss: 1.300961]\n",
            "85 [D loss: 1.061742, acc.: 31.45%] [G loss: 1.046822]\n",
            "86 [D loss: 0.941487, acc.: 40.32%] [G loss: 1.124532]\n",
            "87 [D loss: 0.937069, acc.: 37.10%] [G loss: 0.978535]\n",
            "88 [D loss: 1.015536, acc.: 38.71%] [G loss: 1.141603]\n",
            "89 [D loss: 0.858232, acc.: 44.35%] [G loss: 1.342286]\n",
            "90 [D loss: 0.980509, acc.: 39.52%] [G loss: 1.082250]\n",
            "91 [D loss: 1.053889, acc.: 32.26%] [G loss: 1.159919]\n",
            "92 [D loss: 0.928130, acc.: 36.29%] [G loss: 0.955090]\n",
            "93 [D loss: 0.773709, acc.: 50.00%] [G loss: 1.207903]\n",
            "94 [D loss: 0.861897, acc.: 44.35%] [G loss: 1.144323]\n",
            "95 [D loss: 0.827988, acc.: 49.19%] [G loss: 1.129968]\n",
            "96 [D loss: 0.990788, acc.: 41.13%] [G loss: 1.174052]\n",
            "97 [D loss: 0.964784, acc.: 38.71%] [G loss: 1.210274]\n",
            "98 [D loss: 0.863690, acc.: 45.97%] [G loss: 1.249366]\n",
            "99 [D loss: 1.005503, acc.: 32.26%] [G loss: 1.163242]\n",
            "100 [D loss: 0.899682, acc.: 39.52%] [G loss: 1.026594]\n",
            "101 [D loss: 1.054072, acc.: 33.87%] [G loss: 1.058597]\n",
            "102 [D loss: 0.988784, acc.: 40.32%] [G loss: 1.013384]\n",
            "103 [D loss: 0.994321, acc.: 39.52%] [G loss: 1.053058]\n",
            "104 [D loss: 0.675590, acc.: 61.29%] [G loss: 1.088857]\n",
            "105 [D loss: 0.728701, acc.: 60.48%] [G loss: 0.979588]\n",
            "106 [D loss: 0.736721, acc.: 56.45%] [G loss: 0.967927]\n",
            "107 [D loss: 1.115743, acc.: 27.42%] [G loss: 1.102428]\n",
            "108 [D loss: 0.986952, acc.: 34.68%] [G loss: 1.132487]\n",
            "109 [D loss: 1.060050, acc.: 31.45%] [G loss: 1.225954]\n",
            "110 [D loss: 1.027836, acc.: 32.26%] [G loss: 1.195593]\n",
            "111 [D loss: 0.944275, acc.: 40.32%] [G loss: 1.141732]\n",
            "112 [D loss: 0.911933, acc.: 41.94%] [G loss: 1.207091]\n",
            "113 [D loss: 1.004273, acc.: 30.65%] [G loss: 1.106673]\n",
            "114 [D loss: 0.832151, acc.: 52.42%] [G loss: 0.893773]\n",
            "115 [D loss: 0.899468, acc.: 36.29%] [G loss: 0.952342]\n",
            "116 [D loss: 0.809142, acc.: 43.55%] [G loss: 1.171308]\n",
            "117 [D loss: 0.996205, acc.: 36.29%] [G loss: 1.203025]\n",
            "118 [D loss: 0.934110, acc.: 39.52%] [G loss: 1.169240]\n",
            "119 [D loss: 0.962515, acc.: 37.10%] [G loss: 1.182631]\n",
            "120 [D loss: 0.922763, acc.: 39.52%] [G loss: 1.188686]\n",
            "121 [D loss: 0.867872, acc.: 44.35%] [G loss: 1.038404]\n",
            "122 [D loss: 0.789518, acc.: 51.61%] [G loss: 1.158022]\n",
            "123 [D loss: 0.606412, acc.: 66.13%] [G loss: 1.214379]\n",
            "124 [D loss: 0.783185, acc.: 51.61%] [G loss: 1.060308]\n",
            "125 [D loss: 0.881896, acc.: 52.42%] [G loss: 1.182221]\n",
            "126 [D loss: 0.884434, acc.: 43.55%] [G loss: 1.331997]\n",
            "127 [D loss: 1.021806, acc.: 32.26%] [G loss: 1.239955]\n",
            "128 [D loss: 1.022945, acc.: 30.65%] [G loss: 1.094596]\n",
            "129 [D loss: 0.877322, acc.: 43.55%] [G loss: 1.116393]\n",
            "130 [D loss: 1.002114, acc.: 39.52%] [G loss: 1.039648]\n",
            "131 [D loss: 1.059198, acc.: 31.45%] [G loss: 1.251761]\n",
            "132 [D loss: 0.765298, acc.: 51.61%] [G loss: 1.077494]\n",
            "133 [D loss: 0.870271, acc.: 45.97%] [G loss: 1.151551]\n",
            "134 [D loss: 0.836096, acc.: 54.03%] [G loss: 1.132180]\n",
            "135 [D loss: 0.777450, acc.: 51.61%] [G loss: 1.178806]\n",
            "136 [D loss: 0.963656, acc.: 35.48%] [G loss: 1.011315]\n",
            "137 [D loss: 0.799635, acc.: 53.23%] [G loss: 1.087765]\n",
            "138 [D loss: 0.848720, acc.: 41.94%] [G loss: 1.247597]\n",
            "139 [D loss: 0.851768, acc.: 45.97%] [G loss: 1.201533]\n",
            "140 [D loss: 0.853171, acc.: 49.19%] [G loss: 1.209673]\n",
            "141 [D loss: 0.850150, acc.: 45.97%] [G loss: 1.136663]\n",
            "142 [D loss: 1.035707, acc.: 33.87%] [G loss: 1.080924]\n",
            "143 [D loss: 0.921679, acc.: 39.52%] [G loss: 1.114702]\n",
            "144 [D loss: 0.921265, acc.: 41.13%] [G loss: 1.231939]\n",
            "145 [D loss: 0.852973, acc.: 45.16%] [G loss: 1.133723]\n",
            "146 [D loss: 0.873145, acc.: 42.74%] [G loss: 1.044602]\n",
            "147 [D loss: 0.845559, acc.: 44.35%] [G loss: 1.074595]\n",
            "148 [D loss: 0.668340, acc.: 59.68%] [G loss: 1.017640]\n",
            "149 [D loss: 0.757832, acc.: 52.42%] [G loss: 1.116163]\n",
            "150 [D loss: 0.706609, acc.: 58.06%] [G loss: 0.991625]\n",
            "151 [D loss: 0.840836, acc.: 49.19%] [G loss: 1.105964]\n",
            "152 [D loss: 0.957005, acc.: 38.71%] [G loss: 1.176023]\n",
            "153 [D loss: 0.967188, acc.: 41.13%] [G loss: 1.268820]\n",
            "154 [D loss: 0.943577, acc.: 38.71%] [G loss: 1.236283]\n",
            "155 [D loss: 0.903479, acc.: 42.74%] [G loss: 1.033134]\n",
            "156 [D loss: 0.897125, acc.: 41.94%] [G loss: 1.207492]\n",
            "157 [D loss: 0.845260, acc.: 54.84%] [G loss: 1.160219]\n",
            "158 [D loss: 0.760611, acc.: 53.23%] [G loss: 1.180157]\n",
            "159 [D loss: 0.759949, acc.: 50.81%] [G loss: 1.002518]\n",
            "160 [D loss: 0.856941, acc.: 49.19%] [G loss: 1.039861]\n",
            "161 [D loss: 0.882245, acc.: 46.77%] [G loss: 1.187179]\n",
            "162 [D loss: 0.789734, acc.: 52.42%] [G loss: 1.110027]\n",
            "163 [D loss: 0.900859, acc.: 35.48%] [G loss: 1.117133]\n",
            "164 [D loss: 0.719442, acc.: 54.84%] [G loss: 1.089804]\n",
            "165 [D loss: 0.746105, acc.: 54.03%] [G loss: 1.065031]\n",
            "166 [D loss: 0.787296, acc.: 50.81%] [G loss: 1.020259]\n",
            "167 [D loss: 0.566082, acc.: 71.77%] [G loss: 1.258650]\n",
            "168 [D loss: 0.892813, acc.: 43.55%] [G loss: 1.194635]\n",
            "169 [D loss: 0.960547, acc.: 34.68%] [G loss: 1.376662]\n",
            "170 [D loss: 0.843385, acc.: 46.77%] [G loss: 1.228736]\n",
            "171 [D loss: 0.930372, acc.: 41.94%] [G loss: 1.229824]\n",
            "172 [D loss: 1.142177, acc.: 20.16%] [G loss: 1.113185]\n",
            "173 [D loss: 0.906942, acc.: 41.13%] [G loss: 1.305482]\n",
            "174 [D loss: 0.906734, acc.: 38.71%] [G loss: 1.019463]\n",
            "175 [D loss: 0.932343, acc.: 35.48%] [G loss: 1.073083]\n",
            "176 [D loss: 0.821496, acc.: 49.19%] [G loss: 1.067050]\n",
            "177 [D loss: 0.824359, acc.: 50.81%] [G loss: 1.075169]\n",
            "178 [D loss: 0.872217, acc.: 40.32%] [G loss: 1.012214]\n",
            "179 [D loss: 0.657042, acc.: 59.68%] [G loss: 1.129748]\n",
            "180 [D loss: 0.823717, acc.: 50.81%] [G loss: 1.005329]\n",
            "181 [D loss: 0.768229, acc.: 51.61%] [G loss: 1.069667]\n",
            "182 [D loss: 0.837388, acc.: 45.97%] [G loss: 1.225116]\n",
            "183 [D loss: 0.983398, acc.: 33.87%] [G loss: 1.423849]\n",
            "184 [D loss: 1.088145, acc.: 25.81%] [G loss: 1.124796]\n",
            "185 [D loss: 0.984746, acc.: 34.68%] [G loss: 0.980911]\n",
            "186 [D loss: 0.833430, acc.: 45.16%] [G loss: 1.170432]\n",
            "187 [D loss: 0.866496, acc.: 45.97%] [G loss: 1.345248]\n",
            "188 [D loss: 0.940324, acc.: 41.94%] [G loss: 1.009556]\n",
            "189 [D loss: 0.671550, acc.: 64.52%] [G loss: 1.059282]\n",
            "190 [D loss: 0.750469, acc.: 54.03%] [G loss: 1.111231]\n",
            "191 [D loss: 0.798690, acc.: 56.45%] [G loss: 0.986407]\n",
            "192 [D loss: 0.791462, acc.: 50.00%] [G loss: 1.196668]\n",
            "193 [D loss: 0.993102, acc.: 37.90%] [G loss: 1.157706]\n",
            "194 [D loss: 0.724232, acc.: 58.06%] [G loss: 1.184927]\n",
            "195 [D loss: 0.852856, acc.: 48.39%] [G loss: 1.175993]\n",
            "196 [D loss: 0.980885, acc.: 34.68%] [G loss: 1.065223]\n",
            "197 [D loss: 0.902658, acc.: 47.58%] [G loss: 1.037636]\n",
            "198 [D loss: 1.029252, acc.: 29.03%] [G loss: 0.897691]\n",
            "199 [D loss: 0.958245, acc.: 37.90%] [G loss: 1.191039]\n",
            "200 [D loss: 0.899267, acc.: 44.35%] [G loss: 1.182873]\n",
            "201 [D loss: 0.829278, acc.: 42.74%] [G loss: 1.193987]\n",
            "202 [D loss: 0.915501, acc.: 41.13%] [G loss: 1.243926]\n",
            "203 [D loss: 0.816638, acc.: 47.58%] [G loss: 1.143290]\n",
            "204 [D loss: 0.744569, acc.: 47.58%] [G loss: 1.046193]\n",
            "205 [D loss: 0.771071, acc.: 46.77%] [G loss: 1.089440]\n",
            "206 [D loss: 0.768997, acc.: 51.61%] [G loss: 1.155195]\n",
            "207 [D loss: 0.927913, acc.: 37.10%] [G loss: 1.120478]\n",
            "208 [D loss: 0.849040, acc.: 49.19%] [G loss: 1.126005]\n",
            "209 [D loss: 0.918712, acc.: 44.35%] [G loss: 0.978289]\n",
            "210 [D loss: 0.929476, acc.: 35.48%] [G loss: 1.300830]\n",
            "211 [D loss: 0.921575, acc.: 41.13%] [G loss: 1.256952]\n",
            "212 [D loss: 0.938346, acc.: 40.32%] [G loss: 1.113773]\n",
            "213 [D loss: 0.763208, acc.: 54.84%] [G loss: 1.143133]\n",
            "214 [D loss: 0.835644, acc.: 49.19%] [G loss: 1.046565]\n",
            "215 [D loss: 0.952557, acc.: 39.52%] [G loss: 0.877077]\n",
            "216 [D loss: 0.720018, acc.: 56.45%] [G loss: 0.985813]\n",
            "217 [D loss: 0.852693, acc.: 45.97%] [G loss: 0.974520]\n",
            "218 [D loss: 0.787976, acc.: 51.61%] [G loss: 0.833949]\n",
            "219 [D loss: 0.785436, acc.: 47.58%] [G loss: 0.993588]\n",
            "220 [D loss: 0.795827, acc.: 47.58%] [G loss: 0.890593]\n",
            "221 [D loss: 0.889287, acc.: 45.16%] [G loss: 1.061600]\n",
            "222 [D loss: 0.853862, acc.: 41.13%] [G loss: 1.201320]\n",
            "223 [D loss: 0.900313, acc.: 48.39%] [G loss: 1.123338]\n",
            "224 [D loss: 0.808922, acc.: 48.39%] [G loss: 1.437837]\n",
            "225 [D loss: 1.001888, acc.: 32.26%] [G loss: 1.205565]\n",
            "226 [D loss: 0.921605, acc.: 37.90%] [G loss: 1.171876]\n",
            "227 [D loss: 0.845080, acc.: 49.19%] [G loss: 1.089490]\n",
            "228 [D loss: 0.863618, acc.: 47.58%] [G loss: 0.947741]\n",
            "229 [D loss: 1.028639, acc.: 32.26%] [G loss: 1.041282]\n",
            "230 [D loss: 0.836403, acc.: 49.19%] [G loss: 1.073174]\n",
            "231 [D loss: 0.830587, acc.: 48.39%] [G loss: 1.228114]\n",
            "232 [D loss: 1.003560, acc.: 37.90%] [G loss: 1.017789]\n",
            "233 [D loss: 0.822067, acc.: 43.55%] [G loss: 1.193640]\n",
            "234 [D loss: 0.909267, acc.: 37.90%] [G loss: 1.230095]\n",
            "235 [D loss: 0.901297, acc.: 45.16%] [G loss: 1.140434]\n",
            "236 [D loss: 0.908563, acc.: 39.52%] [G loss: 0.859246]\n",
            "237 [D loss: 0.916778, acc.: 45.97%] [G loss: 1.003422]\n",
            "238 [D loss: 0.905163, acc.: 44.35%] [G loss: 1.000924]\n",
            "239 [D loss: 0.842085, acc.: 42.74%] [G loss: 0.820538]\n",
            "240 [D loss: 0.893460, acc.: 38.71%] [G loss: 0.874761]\n",
            "241 [D loss: 0.860846, acc.: 41.94%] [G loss: 0.976799]\n",
            "242 [D loss: 0.698270, acc.: 56.45%] [G loss: 1.063985]\n",
            "243 [D loss: 0.855972, acc.: 42.74%] [G loss: 1.026897]\n",
            "244 [D loss: 0.830886, acc.: 40.32%] [G loss: 1.038215]\n",
            "245 [D loss: 0.835985, acc.: 41.94%] [G loss: 1.186410]\n",
            "246 [D loss: 0.783231, acc.: 50.00%] [G loss: 1.090046]\n",
            "247 [D loss: 0.902990, acc.: 37.90%] [G loss: 1.170199]\n",
            "248 [D loss: 0.905804, acc.: 41.13%] [G loss: 1.058735]\n",
            "249 [D loss: 0.661104, acc.: 62.10%] [G loss: 1.073279]\n",
            "250 [D loss: 0.945675, acc.: 34.68%] [G loss: 0.933347]\n",
            "251 [D loss: 0.857991, acc.: 44.35%] [G loss: 1.012232]\n",
            "252 [D loss: 0.798439, acc.: 52.42%] [G loss: 1.063847]\n",
            "253 [D loss: 0.899064, acc.: 41.94%] [G loss: 0.961867]\n",
            "254 [D loss: 0.926672, acc.: 37.10%] [G loss: 0.934027]\n",
            "255 [D loss: 0.787258, acc.: 49.19%] [G loss: 0.998374]\n",
            "256 [D loss: 0.948496, acc.: 41.94%] [G loss: 1.071502]\n",
            "257 [D loss: 0.974012, acc.: 33.87%] [G loss: 0.948005]\n",
            "258 [D loss: 0.910112, acc.: 41.13%] [G loss: 1.035305]\n",
            "259 [D loss: 0.889903, acc.: 40.32%] [G loss: 1.240556]\n",
            "260 [D loss: 0.728396, acc.: 53.23%] [G loss: 1.074208]\n",
            "261 [D loss: 0.788491, acc.: 45.97%] [G loss: 0.940839]\n",
            "262 [D loss: 0.702442, acc.: 58.87%] [G loss: 0.944650]\n",
            "263 [D loss: 0.811922, acc.: 46.77%] [G loss: 0.976515]\n",
            "264 [D loss: 0.705888, acc.: 55.65%] [G loss: 0.963686]\n",
            "265 [D loss: 0.975980, acc.: 33.06%] [G loss: 1.022949]\n",
            "266 [D loss: 0.854433, acc.: 46.77%] [G loss: 1.148949]\n",
            "267 [D loss: 1.026467, acc.: 33.06%] [G loss: 1.145593]\n",
            "268 [D loss: 0.967101, acc.: 31.45%] [G loss: 1.064013]\n",
            "269 [D loss: 0.870043, acc.: 41.94%] [G loss: 1.203110]\n",
            "270 [D loss: 0.871679, acc.: 45.16%] [G loss: 1.085905]\n",
            "271 [D loss: 0.859550, acc.: 44.35%] [G loss: 0.916143]\n",
            "272 [D loss: 0.804189, acc.: 53.23%] [G loss: 1.226784]\n",
            "273 [D loss: 0.870296, acc.: 37.10%] [G loss: 0.938979]\n",
            "274 [D loss: 0.755272, acc.: 55.65%] [G loss: 1.023027]\n",
            "275 [D loss: 0.886938, acc.: 46.77%] [G loss: 0.996399]\n",
            "276 [D loss: 0.892601, acc.: 39.52%] [G loss: 1.134350]\n",
            "277 [D loss: 0.875963, acc.: 49.19%] [G loss: 1.059561]\n",
            "278 [D loss: 0.800950, acc.: 46.77%] [G loss: 0.999082]\n",
            "279 [D loss: 0.813078, acc.: 44.35%] [G loss: 1.140713]\n",
            "280 [D loss: 0.707923, acc.: 60.48%] [G loss: 1.011336]\n",
            "281 [D loss: 0.765630, acc.: 51.61%] [G loss: 1.072277]\n",
            "282 [D loss: 0.878933, acc.: 38.71%] [G loss: 0.958315]\n",
            "283 [D loss: 0.726530, acc.: 55.65%] [G loss: 1.059185]\n",
            "284 [D loss: 0.793685, acc.: 47.58%] [G loss: 1.000500]\n",
            "285 [D loss: 0.945937, acc.: 36.29%] [G loss: 0.945221]\n",
            "286 [D loss: 0.823914, acc.: 46.77%] [G loss: 1.035472]\n",
            "287 [D loss: 0.856213, acc.: 37.10%] [G loss: 1.185671]\n",
            "288 [D loss: 0.883395, acc.: 43.55%] [G loss: 1.211400]\n",
            "289 [D loss: 0.772484, acc.: 50.81%] [G loss: 1.035924]\n",
            "290 [D loss: 0.748820, acc.: 50.81%] [G loss: 0.865604]\n",
            "291 [D loss: 0.918074, acc.: 37.90%] [G loss: 1.011825]\n",
            "292 [D loss: 0.667848, acc.: 64.52%] [G loss: 1.218896]\n",
            "293 [D loss: 0.838013, acc.: 45.97%] [G loss: 1.032297]\n",
            "294 [D loss: 0.789784, acc.: 45.97%] [G loss: 1.068790]\n",
            "295 [D loss: 0.807204, acc.: 51.61%] [G loss: 1.166618]\n",
            "296 [D loss: 0.911752, acc.: 41.13%] [G loss: 1.084677]\n",
            "297 [D loss: 0.770170, acc.: 55.65%] [G loss: 1.342664]\n",
            "298 [D loss: 0.879348, acc.: 44.35%] [G loss: 1.074084]\n",
            "299 [D loss: 1.039459, acc.: 28.23%] [G loss: 1.040740]\n",
            "300 [D loss: 0.883244, acc.: 41.94%] [G loss: 1.135187]\n",
            "301 [D loss: 0.825894, acc.: 46.77%] [G loss: 0.988768]\n",
            "302 [D loss: 0.872897, acc.: 45.16%] [G loss: 1.107997]\n",
            "303 [D loss: 0.741604, acc.: 51.61%] [G loss: 1.102443]\n",
            "304 [D loss: 0.917041, acc.: 40.32%] [G loss: 1.087605]\n",
            "305 [D loss: 0.890171, acc.: 42.74%] [G loss: 1.168304]\n",
            "306 [D loss: 0.992398, acc.: 37.10%] [G loss: 1.002725]\n",
            "307 [D loss: 0.902672, acc.: 38.71%] [G loss: 0.940355]\n",
            "308 [D loss: 0.883552, acc.: 40.32%] [G loss: 1.072812]\n",
            "309 [D loss: 0.753880, acc.: 56.45%] [G loss: 0.869491]\n",
            "310 [D loss: 0.718280, acc.: 51.61%] [G loss: 0.896980]\n",
            "311 [D loss: 0.790891, acc.: 45.16%] [G loss: 1.001836]\n",
            "312 [D loss: 1.036574, acc.: 33.87%] [G loss: 1.053185]\n",
            "313 [D loss: 0.946371, acc.: 33.06%] [G loss: 1.052651]\n",
            "314 [D loss: 0.960738, acc.: 37.10%] [G loss: 1.204890]\n",
            "315 [D loss: 0.929732, acc.: 35.48%] [G loss: 1.013605]\n",
            "316 [D loss: 0.808031, acc.: 50.00%] [G loss: 1.061866]\n",
            "317 [D loss: 0.780024, acc.: 50.81%] [G loss: 1.007039]\n",
            "318 [D loss: 0.818843, acc.: 39.52%] [G loss: 0.994917]\n",
            "319 [D loss: 0.751860, acc.: 54.03%] [G loss: 0.895769]\n",
            "320 [D loss: 0.761699, acc.: 56.45%] [G loss: 1.006152]\n",
            "321 [D loss: 0.810963, acc.: 41.94%] [G loss: 0.976918]\n",
            "322 [D loss: 0.817207, acc.: 48.39%] [G loss: 0.919940]\n",
            "323 [D loss: 0.786553, acc.: 50.00%] [G loss: 0.986239]\n",
            "324 [D loss: 0.895722, acc.: 42.74%] [G loss: 0.946671]\n",
            "325 [D loss: 0.909170, acc.: 37.10%] [G loss: 1.037745]\n",
            "326 [D loss: 0.794351, acc.: 46.77%] [G loss: 0.990819]\n",
            "327 [D loss: 0.864655, acc.: 42.74%] [G loss: 1.075326]\n",
            "328 [D loss: 0.858659, acc.: 44.35%] [G loss: 1.043998]\n",
            "329 [D loss: 0.733383, acc.: 54.03%] [G loss: 1.035830]\n",
            "330 [D loss: 0.856957, acc.: 39.52%] [G loss: 0.920812]\n",
            "331 [D loss: 0.923027, acc.: 37.10%] [G loss: 1.049130]\n",
            "332 [D loss: 0.703715, acc.: 57.26%] [G loss: 1.203164]\n",
            "333 [D loss: 0.838362, acc.: 44.35%] [G loss: 0.990268]\n",
            "334 [D loss: 0.865795, acc.: 40.32%] [G loss: 1.068401]\n",
            "335 [D loss: 1.065932, acc.: 26.61%] [G loss: 0.979038]\n",
            "336 [D loss: 0.797601, acc.: 47.58%] [G loss: 1.021750]\n",
            "337 [D loss: 0.743541, acc.: 59.68%] [G loss: 1.042854]\n",
            "338 [D loss: 0.867915, acc.: 41.13%] [G loss: 1.125388]\n",
            "339 [D loss: 0.749476, acc.: 52.42%] [G loss: 1.202270]\n",
            "340 [D loss: 0.927887, acc.: 39.52%] [G loss: 0.985824]\n",
            "341 [D loss: 0.885672, acc.: 41.94%] [G loss: 1.003177]\n",
            "342 [D loss: 1.008790, acc.: 31.45%] [G loss: 1.479140]\n",
            "343 [D loss: 0.826960, acc.: 54.03%] [G loss: 1.131534]\n",
            "344 [D loss: 0.863069, acc.: 47.58%] [G loss: 0.875952]\n",
            "345 [D loss: 0.869376, acc.: 36.29%] [G loss: 0.900059]\n",
            "346 [D loss: 0.718962, acc.: 54.03%] [G loss: 1.122147]\n",
            "347 [D loss: 0.720108, acc.: 58.06%] [G loss: 0.844272]\n",
            "348 [D loss: 0.726434, acc.: 53.23%] [G loss: 1.014310]\n",
            "349 [D loss: 0.810163, acc.: 50.00%] [G loss: 0.951490]\n",
            "350 [D loss: 0.920177, acc.: 43.55%] [G loss: 0.912210]\n",
            "351 [D loss: 0.798299, acc.: 52.42%] [G loss: 1.198901]\n",
            "352 [D loss: 0.795480, acc.: 49.19%] [G loss: 1.081908]\n",
            "353 [D loss: 0.885905, acc.: 42.74%] [G loss: 1.028146]\n",
            "354 [D loss: 0.836492, acc.: 50.00%] [G loss: 0.892116]\n",
            "355 [D loss: 0.827960, acc.: 43.55%] [G loss: 0.994669]\n",
            "356 [D loss: 0.751670, acc.: 47.58%] [G loss: 1.120443]\n",
            "357 [D loss: 0.712987, acc.: 65.32%] [G loss: 1.184758]\n",
            "358 [D loss: 0.842325, acc.: 51.61%] [G loss: 1.106798]\n",
            "359 [D loss: 0.816297, acc.: 49.19%] [G loss: 0.979448]\n",
            "360 [D loss: 0.782701, acc.: 46.77%] [G loss: 0.990802]\n",
            "361 [D loss: 0.833510, acc.: 43.55%] [G loss: 1.046117]\n",
            "362 [D loss: 0.879080, acc.: 40.32%] [G loss: 1.098143]\n",
            "363 [D loss: 0.595229, acc.: 75.00%] [G loss: 1.271516]\n",
            "364 [D loss: 0.718927, acc.: 55.65%] [G loss: 1.139555]\n",
            "365 [D loss: 1.036440, acc.: 36.29%] [G loss: 1.101566]\n",
            "366 [D loss: 1.008378, acc.: 37.10%] [G loss: 1.277817]\n",
            "367 [D loss: 0.809664, acc.: 46.77%] [G loss: 1.174163]\n",
            "368 [D loss: 0.743089, acc.: 51.61%] [G loss: 1.064258]\n",
            "369 [D loss: 0.828933, acc.: 46.77%] [G loss: 0.957364]\n",
            "370 [D loss: 0.705125, acc.: 51.61%] [G loss: 1.019571]\n",
            "371 [D loss: 0.813732, acc.: 46.77%] [G loss: 1.031015]\n",
            "372 [D loss: 0.761206, acc.: 49.19%] [G loss: 1.101650]\n",
            "373 [D loss: 0.787817, acc.: 50.81%] [G loss: 1.080540]\n",
            "374 [D loss: 0.837622, acc.: 47.58%] [G loss: 1.175264]\n",
            "375 [D loss: 0.937151, acc.: 33.87%] [G loss: 1.047675]\n",
            "376 [D loss: 0.988473, acc.: 29.84%] [G loss: 1.179844]\n",
            "377 [D loss: 0.734800, acc.: 51.61%] [G loss: 1.149044]\n",
            "378 [D loss: 0.836905, acc.: 50.81%] [G loss: 1.035162]\n",
            "379 [D loss: 0.814383, acc.: 50.00%] [G loss: 1.069028]\n",
            "380 [D loss: 0.785866, acc.: 48.39%] [G loss: 1.013685]\n",
            "381 [D loss: 0.691770, acc.: 60.48%] [G loss: 0.975706]\n",
            "382 [D loss: 0.774955, acc.: 47.58%] [G loss: 1.016267]\n",
            "383 [D loss: 0.671505, acc.: 59.68%] [G loss: 1.040745]\n",
            "384 [D loss: 0.732569, acc.: 54.03%] [G loss: 1.133417]\n",
            "385 [D loss: 1.018765, acc.: 29.84%] [G loss: 1.226874]\n",
            "386 [D loss: 1.000647, acc.: 38.71%] [G loss: 1.202434]\n",
            "387 [D loss: 0.950884, acc.: 35.48%] [G loss: 1.170473]\n",
            "388 [D loss: 0.753748, acc.: 58.06%] [G loss: 1.163526]\n",
            "389 [D loss: 1.014899, acc.: 29.84%] [G loss: 1.057289]\n",
            "390 [D loss: 0.806049, acc.: 50.00%] [G loss: 1.112108]\n",
            "391 [D loss: 0.817846, acc.: 42.74%] [G loss: 1.141401]\n",
            "392 [D loss: 0.789053, acc.: 47.58%] [G loss: 1.020837]\n",
            "393 [D loss: 0.933986, acc.: 37.90%] [G loss: 1.101776]\n",
            "394 [D loss: 0.821702, acc.: 48.39%] [G loss: 0.910589]\n",
            "395 [D loss: 0.715135, acc.: 51.61%] [G loss: 1.020575]\n",
            "396 [D loss: 0.825232, acc.: 45.16%] [G loss: 0.981376]\n",
            "397 [D loss: 0.823531, acc.: 46.77%] [G loss: 0.987297]\n",
            "398 [D loss: 0.787765, acc.: 49.19%] [G loss: 1.164967]\n",
            "399 [D loss: 0.828550, acc.: 42.74%] [G loss: 0.905782]\n",
            "400 [D loss: 0.783012, acc.: 52.42%] [G loss: 1.119795]\n",
            "401 [D loss: 0.830396, acc.: 47.58%] [G loss: 1.063204]\n",
            "402 [D loss: 0.760003, acc.: 50.00%] [G loss: 1.124744]\n",
            "403 [D loss: 0.808588, acc.: 45.16%] [G loss: 1.087674]\n",
            "404 [D loss: 0.854197, acc.: 41.13%] [G loss: 1.129011]\n",
            "405 [D loss: 0.807610, acc.: 48.39%] [G loss: 1.103070]\n",
            "406 [D loss: 0.746770, acc.: 52.42%] [G loss: 1.125699]\n",
            "407 [D loss: 0.868353, acc.: 45.97%] [G loss: 1.069707]\n",
            "408 [D loss: 0.845500, acc.: 42.74%] [G loss: 0.935238]\n",
            "409 [D loss: 0.805092, acc.: 47.58%] [G loss: 1.159275]\n",
            "410 [D loss: 0.807791, acc.: 44.35%] [G loss: 1.072208]\n",
            "411 [D loss: 0.776097, acc.: 54.03%] [G loss: 1.084175]\n",
            "412 [D loss: 0.681278, acc.: 58.06%] [G loss: 0.885744]\n",
            "413 [D loss: 0.798096, acc.: 46.77%] [G loss: 0.977312]\n",
            "414 [D loss: 0.865391, acc.: 41.94%] [G loss: 1.064322]\n",
            "415 [D loss: 0.773891, acc.: 44.35%] [G loss: 1.062904]\n",
            "416 [D loss: 0.920521, acc.: 38.71%] [G loss: 1.012727]\n",
            "417 [D loss: 0.747900, acc.: 53.23%] [G loss: 0.919414]\n",
            "418 [D loss: 0.894463, acc.: 45.16%] [G loss: 1.040811]\n",
            "419 [D loss: 0.736837, acc.: 58.06%] [G loss: 1.198837]\n",
            "420 [D loss: 0.809302, acc.: 48.39%] [G loss: 1.184383]\n",
            "421 [D loss: 0.894008, acc.: 43.55%] [G loss: 1.156196]\n",
            "422 [D loss: 0.881493, acc.: 45.97%] [G loss: 1.127245]\n",
            "423 [D loss: 0.840310, acc.: 50.00%] [G loss: 1.063140]\n",
            "424 [D loss: 0.812141, acc.: 47.58%] [G loss: 1.021344]\n",
            "425 [D loss: 0.792330, acc.: 50.00%] [G loss: 1.045778]\n",
            "426 [D loss: 0.697237, acc.: 54.84%] [G loss: 1.102036]\n",
            "427 [D loss: 0.915427, acc.: 37.10%] [G loss: 1.070768]\n",
            "428 [D loss: 0.791606, acc.: 49.19%] [G loss: 1.184244]\n",
            "429 [D loss: 0.902467, acc.: 39.52%] [G loss: 0.970793]\n",
            "430 [D loss: 0.830164, acc.: 44.35%] [G loss: 0.978730]\n",
            "431 [D loss: 0.768944, acc.: 45.97%] [G loss: 0.993583]\n",
            "432 [D loss: 0.862935, acc.: 45.97%] [G loss: 1.008120]\n",
            "433 [D loss: 0.737725, acc.: 51.61%] [G loss: 1.131361]\n",
            "434 [D loss: 0.824014, acc.: 45.16%] [G loss: 1.038731]\n",
            "435 [D loss: 0.848225, acc.: 44.35%] [G loss: 1.147821]\n",
            "436 [D loss: 0.841848, acc.: 45.97%] [G loss: 1.193185]\n",
            "437 [D loss: 0.832860, acc.: 42.74%] [G loss: 1.000877]\n",
            "438 [D loss: 0.767403, acc.: 50.81%] [G loss: 1.029357]\n",
            "439 [D loss: 0.856637, acc.: 43.55%] [G loss: 0.941817]\n",
            "440 [D loss: 0.824237, acc.: 45.16%] [G loss: 1.192396]\n",
            "441 [D loss: 0.832518, acc.: 48.39%] [G loss: 0.922859]\n",
            "442 [D loss: 0.917001, acc.: 45.16%] [G loss: 0.996514]\n",
            "443 [D loss: 0.818017, acc.: 42.74%] [G loss: 1.056449]\n",
            "444 [D loss: 0.829213, acc.: 45.97%] [G loss: 1.097285]\n",
            "445 [D loss: 0.824571, acc.: 45.97%] [G loss: 1.025982]\n",
            "446 [D loss: 0.823916, acc.: 42.74%] [G loss: 1.018870]\n",
            "447 [D loss: 0.742064, acc.: 48.39%] [G loss: 1.047299]\n",
            "448 [D loss: 0.795385, acc.: 48.39%] [G loss: 1.009362]\n",
            "449 [D loss: 0.727508, acc.: 60.48%] [G loss: 0.996214]\n",
            "450 [D loss: 0.720344, acc.: 58.87%] [G loss: 1.091367]\n",
            "451 [D loss: 0.800721, acc.: 50.00%] [G loss: 1.123762]\n",
            "452 [D loss: 0.710313, acc.: 56.45%] [G loss: 1.153901]\n",
            "453 [D loss: 0.821446, acc.: 45.16%] [G loss: 1.038658]\n",
            "454 [D loss: 0.749669, acc.: 50.81%] [G loss: 1.222964]\n",
            "455 [D loss: 0.861275, acc.: 43.55%] [G loss: 1.271114]\n",
            "456 [D loss: 0.729787, acc.: 55.65%] [G loss: 1.195205]\n",
            "457 [D loss: 0.895133, acc.: 36.29%] [G loss: 0.982156]\n",
            "458 [D loss: 0.898549, acc.: 41.13%] [G loss: 1.014400]\n",
            "459 [D loss: 0.727660, acc.: 53.23%] [G loss: 1.062737]\n",
            "460 [D loss: 0.757624, acc.: 55.65%] [G loss: 1.005660]\n",
            "461 [D loss: 0.822404, acc.: 47.58%] [G loss: 0.866151]\n",
            "462 [D loss: 0.829136, acc.: 42.74%] [G loss: 0.978864]\n",
            "463 [D loss: 0.698340, acc.: 56.45%] [G loss: 0.989873]\n",
            "464 [D loss: 0.668362, acc.: 58.06%] [G loss: 0.956202]\n",
            "465 [D loss: 0.770707, acc.: 53.23%] [G loss: 1.164291]\n",
            "466 [D loss: 0.613300, acc.: 62.90%] [G loss: 1.127206]\n",
            "467 [D loss: 0.815594, acc.: 51.61%] [G loss: 1.170721]\n",
            "468 [D loss: 0.802419, acc.: 42.74%] [G loss: 0.969118]\n",
            "469 [D loss: 0.897522, acc.: 41.13%] [G loss: 1.132168]\n",
            "470 [D loss: 0.887139, acc.: 38.71%] [G loss: 1.100963]\n",
            "471 [D loss: 0.771946, acc.: 50.81%] [G loss: 0.947297]\n",
            "472 [D loss: 0.793730, acc.: 53.23%] [G loss: 1.021154]\n",
            "473 [D loss: 0.742529, acc.: 54.03%] [G loss: 0.943594]\n",
            "474 [D loss: 0.790103, acc.: 46.77%] [G loss: 0.864524]\n",
            "475 [D loss: 0.784389, acc.: 47.58%] [G loss: 0.911285]\n",
            "476 [D loss: 0.857408, acc.: 41.94%] [G loss: 1.022483]\n",
            "477 [D loss: 0.800883, acc.: 48.39%] [G loss: 1.106684]\n",
            "478 [D loss: 0.882587, acc.: 40.32%] [G loss: 1.268914]\n",
            "479 [D loss: 0.735055, acc.: 55.65%] [G loss: 0.976065]\n",
            "480 [D loss: 0.917791, acc.: 37.10%] [G loss: 0.968231]\n",
            "481 [D loss: 0.861796, acc.: 41.13%] [G loss: 1.083185]\n",
            "482 [D loss: 0.822014, acc.: 50.81%] [G loss: 0.968937]\n",
            "483 [D loss: 0.912045, acc.: 33.87%] [G loss: 0.996069]\n",
            "484 [D loss: 0.810868, acc.: 46.77%] [G loss: 1.013038]\n",
            "485 [D loss: 0.727387, acc.: 54.03%] [G loss: 1.117349]\n",
            "486 [D loss: 0.810586, acc.: 45.97%] [G loss: 1.052884]\n",
            "487 [D loss: 0.754200, acc.: 50.81%] [G loss: 1.019665]\n",
            "488 [D loss: 0.744181, acc.: 63.71%] [G loss: 1.107483]\n",
            "489 [D loss: 0.748200, acc.: 50.00%] [G loss: 0.977727]\n",
            "490 [D loss: 0.779883, acc.: 49.19%] [G loss: 1.025223]\n",
            "491 [D loss: 0.738653, acc.: 52.42%] [G loss: 1.011355]\n",
            "492 [D loss: 0.813492, acc.: 45.97%] [G loss: 1.165880]\n",
            "493 [D loss: 0.849861, acc.: 44.35%] [G loss: 1.096411]\n",
            "494 [D loss: 0.883506, acc.: 37.90%] [G loss: 1.225379]\n",
            "495 [D loss: 0.821763, acc.: 41.94%] [G loss: 1.153244]\n",
            "496 [D loss: 0.863601, acc.: 44.35%] [G loss: 1.001022]\n",
            "497 [D loss: 0.956854, acc.: 34.68%] [G loss: 0.946463]\n",
            "498 [D loss: 0.764210, acc.: 54.84%] [G loss: 1.101144]\n",
            "499 [D loss: 0.799704, acc.: 47.58%] [G loss: 1.050728]\n",
            "500 [D loss: 0.723846, acc.: 55.65%] [G loss: 1.001606]\n",
            "501 [D loss: 0.794627, acc.: 48.39%] [G loss: 0.921940]\n",
            "502 [D loss: 0.774101, acc.: 54.03%] [G loss: 0.945713]\n",
            "503 [D loss: 0.649092, acc.: 61.29%] [G loss: 1.060117]\n",
            "504 [D loss: 0.733289, acc.: 54.84%] [G loss: 1.082502]\n",
            "505 [D loss: 0.921656, acc.: 43.55%] [G loss: 0.929378]\n",
            "506 [D loss: 0.764838, acc.: 50.00%] [G loss: 1.047811]\n",
            "507 [D loss: 0.875677, acc.: 37.90%] [G loss: 1.057012]\n",
            "508 [D loss: 0.733193, acc.: 51.61%] [G loss: 1.090428]\n",
            "509 [D loss: 0.766229, acc.: 47.58%] [G loss: 1.080596]\n",
            "510 [D loss: 0.703178, acc.: 58.87%] [G loss: 1.053554]\n",
            "511 [D loss: 0.693484, acc.: 58.06%] [G loss: 1.024681]\n",
            "512 [D loss: 0.689721, acc.: 54.84%] [G loss: 1.099930]\n",
            "513 [D loss: 0.815466, acc.: 49.19%] [G loss: 1.118009]\n",
            "514 [D loss: 0.824977, acc.: 46.77%] [G loss: 1.311931]\n",
            "515 [D loss: 1.002455, acc.: 37.90%] [G loss: 1.065652]\n",
            "516 [D loss: 0.799465, acc.: 55.65%] [G loss: 1.215172]\n",
            "517 [D loss: 0.826361, acc.: 42.74%] [G loss: 1.221911]\n",
            "518 [D loss: 0.972163, acc.: 31.45%] [G loss: 1.064265]\n",
            "519 [D loss: 0.832173, acc.: 43.55%] [G loss: 1.126613]\n",
            "520 [D loss: 0.753254, acc.: 54.03%] [G loss: 1.043673]\n",
            "521 [D loss: 0.795587, acc.: 47.58%] [G loss: 1.002600]\n",
            "522 [D loss: 0.684051, acc.: 55.65%] [G loss: 0.918131]\n",
            "523 [D loss: 0.819176, acc.: 48.39%] [G loss: 1.011069]\n",
            "524 [D loss: 0.820957, acc.: 44.35%] [G loss: 0.924551]\n",
            "525 [D loss: 0.844103, acc.: 36.29%] [G loss: 1.140392]\n",
            "526 [D loss: 0.858940, acc.: 42.74%] [G loss: 1.095020]\n",
            "527 [D loss: 0.775967, acc.: 52.42%] [G loss: 1.031291]\n",
            "528 [D loss: 0.885907, acc.: 45.16%] [G loss: 1.031242]\n",
            "529 [D loss: 0.801497, acc.: 53.23%] [G loss: 0.968742]\n",
            "530 [D loss: 0.707872, acc.: 58.87%] [G loss: 0.992657]\n",
            "531 [D loss: 0.753644, acc.: 52.42%] [G loss: 1.072376]\n",
            "532 [D loss: 0.750631, acc.: 51.61%] [G loss: 1.054878]\n",
            "533 [D loss: 0.650615, acc.: 66.13%] [G loss: 1.166720]\n",
            "534 [D loss: 0.827732, acc.: 48.39%] [G loss: 1.057305]\n",
            "535 [D loss: 0.834414, acc.: 42.74%] [G loss: 1.121544]\n",
            "536 [D loss: 0.906247, acc.: 40.32%] [G loss: 1.015712]\n",
            "537 [D loss: 0.936107, acc.: 38.71%] [G loss: 1.057224]\n",
            "538 [D loss: 0.854515, acc.: 40.32%] [G loss: 1.167109]\n",
            "539 [D loss: 0.821289, acc.: 50.81%] [G loss: 0.965229]\n",
            "540 [D loss: 0.819792, acc.: 44.35%] [G loss: 1.118863]\n",
            "541 [D loss: 0.780242, acc.: 51.61%] [G loss: 1.004883]\n",
            "542 [D loss: 0.727742, acc.: 56.45%] [G loss: 0.945517]\n",
            "543 [D loss: 0.670368, acc.: 58.87%] [G loss: 0.954436]\n",
            "544 [D loss: 0.708796, acc.: 55.65%] [G loss: 0.941617]\n",
            "545 [D loss: 0.754357, acc.: 54.03%] [G loss: 0.888812]\n",
            "546 [D loss: 0.732321, acc.: 56.45%] [G loss: 0.953009]\n",
            "547 [D loss: 0.731231, acc.: 56.45%] [G loss: 0.917560]\n",
            "548 [D loss: 0.817869, acc.: 42.74%] [G loss: 1.007865]\n",
            "549 [D loss: 0.742182, acc.: 54.84%] [G loss: 1.157089]\n",
            "550 [D loss: 0.886480, acc.: 44.35%] [G loss: 1.106599]\n",
            "551 [D loss: 0.810365, acc.: 46.77%] [G loss: 1.019671]\n",
            "552 [D loss: 0.869942, acc.: 48.39%] [G loss: 1.176939]\n",
            "553 [D loss: 0.725424, acc.: 56.45%] [G loss: 1.152507]\n",
            "554 [D loss: 0.839025, acc.: 39.52%] [G loss: 0.995859]\n",
            "555 [D loss: 0.689580, acc.: 56.45%] [G loss: 1.045825]\n",
            "556 [D loss: 0.793052, acc.: 47.58%] [G loss: 1.088611]\n",
            "557 [D loss: 0.718219, acc.: 57.26%] [G loss: 0.957630]\n",
            "558 [D loss: 0.800294, acc.: 49.19%] [G loss: 0.989075]\n",
            "559 [D loss: 0.715478, acc.: 58.87%] [G loss: 1.023390]\n",
            "560 [D loss: 0.802737, acc.: 51.61%] [G loss: 0.868784]\n",
            "561 [D loss: 0.718266, acc.: 58.87%] [G loss: 1.129372]\n",
            "562 [D loss: 0.850259, acc.: 45.97%] [G loss: 1.137561]\n",
            "563 [D loss: 0.782615, acc.: 48.39%] [G loss: 1.169774]\n",
            "564 [D loss: 0.899469, acc.: 38.71%] [G loss: 1.021599]\n",
            "565 [D loss: 0.825740, acc.: 46.77%] [G loss: 1.087564]\n",
            "566 [D loss: 0.854637, acc.: 41.13%] [G loss: 1.057718]\n",
            "567 [D loss: 0.854487, acc.: 42.74%] [G loss: 1.030467]\n",
            "568 [D loss: 0.744707, acc.: 53.23%] [G loss: 1.105130]\n",
            "569 [D loss: 0.876464, acc.: 40.32%] [G loss: 1.046200]\n",
            "570 [D loss: 0.771780, acc.: 46.77%] [G loss: 1.039889]\n",
            "571 [D loss: 0.767914, acc.: 51.61%] [G loss: 1.072164]\n",
            "572 [D loss: 0.760372, acc.: 50.81%] [G loss: 0.980089]\n",
            "573 [D loss: 0.902928, acc.: 44.35%] [G loss: 1.043362]\n",
            "574 [D loss: 0.751745, acc.: 55.65%] [G loss: 1.003500]\n",
            "575 [D loss: 0.667725, acc.: 58.06%] [G loss: 0.992804]\n",
            "576 [D loss: 0.682636, acc.: 59.68%] [G loss: 1.027717]\n",
            "577 [D loss: 0.778273, acc.: 51.61%] [G loss: 1.029600]\n",
            "578 [D loss: 0.727445, acc.: 58.06%] [G loss: 1.000383]\n",
            "579 [D loss: 0.815022, acc.: 44.35%] [G loss: 1.111707]\n",
            "580 [D loss: 0.810119, acc.: 46.77%] [G loss: 0.920090]\n",
            "581 [D loss: 0.868398, acc.: 42.74%] [G loss: 0.990191]\n",
            "582 [D loss: 0.801823, acc.: 44.35%] [G loss: 1.133257]\n",
            "583 [D loss: 0.776604, acc.: 49.19%] [G loss: 1.014345]\n",
            "584 [D loss: 0.749116, acc.: 53.23%] [G loss: 1.095393]\n",
            "585 [D loss: 0.865761, acc.: 38.71%] [G loss: 1.018794]\n",
            "586 [D loss: 0.839557, acc.: 46.77%] [G loss: 1.010803]\n",
            "587 [D loss: 0.790706, acc.: 45.16%] [G loss: 1.022864]\n",
            "588 [D loss: 0.741747, acc.: 58.87%] [G loss: 1.033707]\n",
            "589 [D loss: 0.856383, acc.: 45.97%] [G loss: 1.016790]\n",
            "590 [D loss: 0.845864, acc.: 43.55%] [G loss: 0.939054]\n",
            "591 [D loss: 0.760778, acc.: 46.77%] [G loss: 1.038028]\n",
            "592 [D loss: 0.780337, acc.: 45.16%] [G loss: 1.080780]\n",
            "593 [D loss: 0.811434, acc.: 52.42%] [G loss: 0.993309]\n",
            "594 [D loss: 0.702395, acc.: 57.26%] [G loss: 0.983094]\n",
            "595 [D loss: 0.712716, acc.: 58.06%] [G loss: 0.939156]\n",
            "596 [D loss: 0.726509, acc.: 56.45%] [G loss: 1.040161]\n",
            "597 [D loss: 0.758997, acc.: 51.61%] [G loss: 1.033107]\n",
            "598 [D loss: 0.828346, acc.: 44.35%] [G loss: 1.065268]\n",
            "599 [D loss: 0.743059, acc.: 53.23%] [G loss: 1.075160]\n",
            "600 [D loss: 0.815938, acc.: 49.19%] [G loss: 1.085220]\n",
            "601 [D loss: 0.933417, acc.: 37.90%] [G loss: 0.981947]\n",
            "602 [D loss: 0.936939, acc.: 41.94%] [G loss: 1.113346]\n",
            "603 [D loss: 0.802013, acc.: 45.97%] [G loss: 1.084500]\n",
            "604 [D loss: 0.829504, acc.: 41.94%] [G loss: 1.122446]\n",
            "605 [D loss: 0.751060, acc.: 50.00%] [G loss: 1.126911]\n",
            "606 [D loss: 0.787770, acc.: 51.61%] [G loss: 0.921995]\n",
            "607 [D loss: 0.769982, acc.: 50.00%] [G loss: 0.994792]\n",
            "608 [D loss: 0.755188, acc.: 54.84%] [G loss: 1.100222]\n",
            "609 [D loss: 0.790810, acc.: 47.58%] [G loss: 0.999061]\n",
            "610 [D loss: 0.784190, acc.: 50.00%] [G loss: 1.111708]\n",
            "611 [D loss: 0.772973, acc.: 42.74%] [G loss: 1.069809]\n",
            "612 [D loss: 0.813612, acc.: 45.97%] [G loss: 1.069576]\n",
            "613 [D loss: 0.856782, acc.: 41.94%] [G loss: 1.056582]\n",
            "614 [D loss: 0.857120, acc.: 43.55%] [G loss: 1.086659]\n",
            "615 [D loss: 0.841908, acc.: 45.16%] [G loss: 1.154218]\n",
            "616 [D loss: 0.843042, acc.: 49.19%] [G loss: 0.976939]\n",
            "617 [D loss: 0.776232, acc.: 47.58%] [G loss: 1.048317]\n",
            "618 [D loss: 0.758747, acc.: 54.84%] [G loss: 0.989699]\n",
            "619 [D loss: 0.735467, acc.: 52.42%] [G loss: 0.986657]\n",
            "620 [D loss: 0.760290, acc.: 46.77%] [G loss: 0.939022]\n",
            "621 [D loss: 0.779317, acc.: 50.00%] [G loss: 1.036595]\n",
            "622 [D loss: 0.791275, acc.: 46.77%] [G loss: 0.987028]\n",
            "623 [D loss: 0.753467, acc.: 55.65%] [G loss: 0.982568]\n",
            "624 [D loss: 0.726297, acc.: 57.26%] [G loss: 1.006487]\n",
            "625 [D loss: 0.714555, acc.: 54.03%] [G loss: 0.996367]\n",
            "626 [D loss: 0.763524, acc.: 49.19%] [G loss: 0.974623]\n",
            "627 [D loss: 0.818285, acc.: 48.39%] [G loss: 1.031357]\n",
            "628 [D loss: 0.707812, acc.: 54.03%] [G loss: 0.963733]\n",
            "629 [D loss: 0.864308, acc.: 41.94%] [G loss: 1.089988]\n",
            "630 [D loss: 0.853135, acc.: 45.97%] [G loss: 1.093485]\n",
            "631 [D loss: 0.742265, acc.: 55.65%] [G loss: 1.134110]\n",
            "632 [D loss: 0.817607, acc.: 41.13%] [G loss: 0.949248]\n",
            "633 [D loss: 0.768765, acc.: 53.23%] [G loss: 1.090661]\n",
            "634 [D loss: 0.766387, acc.: 50.81%] [G loss: 1.054402]\n",
            "635 [D loss: 0.814336, acc.: 43.55%] [G loss: 0.864422]\n",
            "636 [D loss: 0.734729, acc.: 54.84%] [G loss: 0.999683]\n",
            "637 [D loss: 0.725333, acc.: 58.87%] [G loss: 0.920383]\n",
            "638 [D loss: 0.747692, acc.: 53.23%] [G loss: 0.981386]\n",
            "639 [D loss: 0.786245, acc.: 47.58%] [G loss: 0.928501]\n",
            "640 [D loss: 0.714526, acc.: 51.61%] [G loss: 0.939471]\n",
            "641 [D loss: 0.791516, acc.: 46.77%] [G loss: 0.965578]\n",
            "642 [D loss: 0.700879, acc.: 58.06%] [G loss: 0.993767]\n",
            "643 [D loss: 0.612747, acc.: 61.29%] [G loss: 1.052992]\n",
            "644 [D loss: 0.762633, acc.: 50.81%] [G loss: 0.940381]\n",
            "645 [D loss: 0.717122, acc.: 50.81%] [G loss: 1.082740]\n",
            "646 [D loss: 0.735141, acc.: 51.61%] [G loss: 1.118636]\n",
            "647 [D loss: 0.814267, acc.: 45.97%] [G loss: 1.039327]\n",
            "648 [D loss: 0.845585, acc.: 41.13%] [G loss: 1.203921]\n",
            "649 [D loss: 0.735100, acc.: 59.68%] [G loss: 0.969877]\n",
            "650 [D loss: 0.673303, acc.: 59.68%] [G loss: 1.068647]\n",
            "651 [D loss: 0.699502, acc.: 58.06%] [G loss: 0.997104]\n",
            "652 [D loss: 0.743536, acc.: 53.23%] [G loss: 0.823119]\n",
            "653 [D loss: 0.655637, acc.: 62.90%] [G loss: 0.928155]\n",
            "654 [D loss: 0.771290, acc.: 50.81%] [G loss: 0.996203]\n",
            "655 [D loss: 0.677012, acc.: 58.06%] [G loss: 1.066154]\n",
            "656 [D loss: 0.800904, acc.: 44.35%] [G loss: 1.063115]\n",
            "657 [D loss: 0.722806, acc.: 54.03%] [G loss: 0.865167]\n",
            "658 [D loss: 0.813967, acc.: 45.16%] [G loss: 1.035776]\n",
            "659 [D loss: 0.767910, acc.: 50.81%] [G loss: 0.996303]\n",
            "660 [D loss: 0.694556, acc.: 54.84%] [G loss: 1.018496]\n",
            "661 [D loss: 0.760810, acc.: 49.19%] [G loss: 1.041667]\n",
            "662 [D loss: 0.700966, acc.: 54.84%] [G loss: 1.003283]\n",
            "663 [D loss: 0.661548, acc.: 60.48%] [G loss: 1.032852]\n",
            "664 [D loss: 0.767195, acc.: 48.39%] [G loss: 1.094292]\n",
            "665 [D loss: 0.852850, acc.: 46.77%] [G loss: 1.027108]\n",
            "666 [D loss: 0.759054, acc.: 54.03%] [G loss: 1.065110]\n",
            "667 [D loss: 0.765548, acc.: 58.06%] [G loss: 1.175871]\n",
            "668 [D loss: 0.887603, acc.: 42.74%] [G loss: 1.210902]\n",
            "669 [D loss: 0.673290, acc.: 58.87%] [G loss: 1.219116]\n",
            "670 [D loss: 0.819215, acc.: 52.42%] [G loss: 1.105448]\n",
            "671 [D loss: 0.786424, acc.: 53.23%] [G loss: 0.957413]\n",
            "672 [D loss: 0.736044, acc.: 53.23%] [G loss: 1.156728]\n",
            "673 [D loss: 0.852398, acc.: 44.35%] [G loss: 1.037276]\n",
            "674 [D loss: 0.749084, acc.: 47.58%] [G loss: 1.123148]\n",
            "675 [D loss: 0.667821, acc.: 62.10%] [G loss: 0.988454]\n",
            "676 [D loss: 0.715320, acc.: 54.84%] [G loss: 0.939016]\n",
            "677 [D loss: 0.699794, acc.: 56.45%] [G loss: 0.983898]\n",
            "678 [D loss: 0.675521, acc.: 55.65%] [G loss: 1.045986]\n",
            "679 [D loss: 0.738528, acc.: 49.19%] [G loss: 0.919247]\n",
            "680 [D loss: 0.755607, acc.: 52.42%] [G loss: 1.059843]\n",
            "681 [D loss: 0.795892, acc.: 45.97%] [G loss: 1.125126]\n",
            "682 [D loss: 0.729513, acc.: 54.03%] [G loss: 1.066500]\n",
            "683 [D loss: 0.779413, acc.: 52.42%] [G loss: 1.186740]\n",
            "684 [D loss: 0.765884, acc.: 47.58%] [G loss: 1.035613]\n",
            "685 [D loss: 0.642765, acc.: 68.55%] [G loss: 1.060708]\n",
            "686 [D loss: 0.728780, acc.: 57.26%] [G loss: 0.970826]\n",
            "687 [D loss: 0.696568, acc.: 55.65%] [G loss: 1.012887]\n",
            "688 [D loss: 0.717714, acc.: 56.45%] [G loss: 0.969951]\n",
            "689 [D loss: 0.748967, acc.: 51.61%] [G loss: 0.937062]\n",
            "690 [D loss: 0.766288, acc.: 50.00%] [G loss: 1.049393]\n",
            "691 [D loss: 0.709258, acc.: 56.45%] [G loss: 1.135699]\n",
            "692 [D loss: 0.714052, acc.: 56.45%] [G loss: 1.067003]\n",
            "693 [D loss: 0.681027, acc.: 62.90%] [G loss: 1.106451]\n",
            "694 [D loss: 0.781860, acc.: 46.77%] [G loss: 1.088159]\n",
            "695 [D loss: 0.768781, acc.: 48.39%] [G loss: 1.072695]\n",
            "696 [D loss: 0.729336, acc.: 54.03%] [G loss: 1.153885]\n",
            "697 [D loss: 0.762275, acc.: 48.39%] [G loss: 1.066423]\n",
            "698 [D loss: 0.723252, acc.: 54.03%] [G loss: 0.861182]\n",
            "699 [D loss: 0.636690, acc.: 59.68%] [G loss: 0.936352]\n",
            "700 [D loss: 0.746154, acc.: 57.26%] [G loss: 1.005983]\n",
            "701 [D loss: 0.680817, acc.: 62.10%] [G loss: 0.956795]\n",
            "702 [D loss: 0.640845, acc.: 62.90%] [G loss: 1.067121]\n",
            "703 [D loss: 0.757026, acc.: 51.61%] [G loss: 1.075580]\n",
            "704 [D loss: 0.669032, acc.: 62.10%] [G loss: 0.869211]\n",
            "705 [D loss: 0.725559, acc.: 53.23%] [G loss: 1.049060]\n",
            "706 [D loss: 0.702327, acc.: 57.26%] [G loss: 1.021978]\n",
            "707 [D loss: 0.675176, acc.: 61.29%] [G loss: 0.930866]\n",
            "708 [D loss: 0.738776, acc.: 52.42%] [G loss: 0.964925]\n",
            "709 [D loss: 0.657380, acc.: 65.32%] [G loss: 0.971725]\n",
            "710 [D loss: 0.711259, acc.: 57.26%] [G loss: 0.990657]\n",
            "711 [D loss: 0.697364, acc.: 60.48%] [G loss: 1.001541]\n",
            "712 [D loss: 0.781598, acc.: 50.81%] [G loss: 1.095963]\n",
            "713 [D loss: 0.638705, acc.: 66.94%] [G loss: 1.099674]\n",
            "714 [D loss: 0.817194, acc.: 50.00%] [G loss: 1.062359]\n",
            "715 [D loss: 0.757169, acc.: 54.84%] [G loss: 0.914791]\n",
            "716 [D loss: 0.629464, acc.: 65.32%] [G loss: 1.051171]\n",
            "717 [D loss: 0.619247, acc.: 64.52%] [G loss: 0.991757]\n",
            "718 [D loss: 0.752204, acc.: 50.00%] [G loss: 1.000285]\n",
            "719 [D loss: 0.663965, acc.: 59.68%] [G loss: 0.959232]\n",
            "720 [D loss: 0.675330, acc.: 58.06%] [G loss: 1.025835]\n",
            "721 [D loss: 0.640359, acc.: 65.32%] [G loss: 1.092047]\n",
            "722 [D loss: 0.714159, acc.: 56.45%] [G loss: 1.059370]\n",
            "723 [D loss: 0.682104, acc.: 54.03%] [G loss: 1.082838]\n",
            "724 [D loss: 0.610874, acc.: 66.13%] [G loss: 1.117381]\n",
            "725 [D loss: 0.722964, acc.: 51.61%] [G loss: 1.041078]\n",
            "726 [D loss: 0.552280, acc.: 70.97%] [G loss: 1.208432]\n",
            "727 [D loss: 0.692722, acc.: 61.29%] [G loss: 1.023736]\n",
            "728 [D loss: 0.547117, acc.: 70.97%] [G loss: 1.094061]\n",
            "729 [D loss: 0.674622, acc.: 61.29%] [G loss: 1.076638]\n",
            "730 [D loss: 0.689070, acc.: 58.06%] [G loss: 0.977842]\n",
            "731 [D loss: 0.672788, acc.: 60.48%] [G loss: 0.986469]\n",
            "732 [D loss: 0.733687, acc.: 52.42%] [G loss: 1.122020]\n",
            "733 [D loss: 0.666477, acc.: 59.68%] [G loss: 1.102835]\n",
            "734 [D loss: 0.677889, acc.: 60.48%] [G loss: 0.944532]\n",
            "735 [D loss: 0.698204, acc.: 56.45%] [G loss: 1.067356]\n",
            "736 [D loss: 0.673482, acc.: 63.71%] [G loss: 1.111870]\n",
            "737 [D loss: 0.786007, acc.: 51.61%] [G loss: 0.918310]\n",
            "738 [D loss: 0.628080, acc.: 64.52%] [G loss: 1.130595]\n",
            "739 [D loss: 0.646552, acc.: 62.90%] [G loss: 1.063431]\n",
            "740 [D loss: 0.705776, acc.: 54.84%] [G loss: 1.065045]\n",
            "741 [D loss: 0.730904, acc.: 54.03%] [G loss: 1.097375]\n",
            "742 [D loss: 0.645756, acc.: 61.29%] [G loss: 1.054239]\n",
            "743 [D loss: 0.763902, acc.: 55.65%] [G loss: 1.048478]\n",
            "744 [D loss: 0.665602, acc.: 58.87%] [G loss: 1.105772]\n",
            "745 [D loss: 0.731234, acc.: 55.65%] [G loss: 0.966621]\n",
            "746 [D loss: 0.741448, acc.: 52.42%] [G loss: 1.040109]\n",
            "747 [D loss: 0.639186, acc.: 62.90%] [G loss: 1.013156]\n",
            "748 [D loss: 0.631729, acc.: 62.10%] [G loss: 1.008892]\n",
            "749 [D loss: 0.544592, acc.: 73.39%] [G loss: 1.214520]\n",
            "750 [D loss: 0.678031, acc.: 58.87%] [G loss: 1.098190]\n",
            "751 [D loss: 0.673636, acc.: 56.45%] [G loss: 0.967602]\n",
            "752 [D loss: 0.675857, acc.: 58.87%] [G loss: 1.015367]\n",
            "753 [D loss: 0.647197, acc.: 63.71%] [G loss: 1.027917]\n",
            "754 [D loss: 0.548012, acc.: 75.81%] [G loss: 1.122192]\n",
            "755 [D loss: 0.575303, acc.: 70.16%] [G loss: 1.031201]\n",
            "756 [D loss: 0.751914, acc.: 49.19%] [G loss: 1.017806]\n",
            "757 [D loss: 0.559738, acc.: 70.97%] [G loss: 0.984339]\n",
            "758 [D loss: 0.574117, acc.: 70.97%] [G loss: 1.071576]\n",
            "759 [D loss: 0.693807, acc.: 58.06%] [G loss: 1.021359]\n",
            "760 [D loss: 0.559002, acc.: 67.74%] [G loss: 1.058606]\n",
            "761 [D loss: 0.702141, acc.: 59.68%] [G loss: 1.114153]\n",
            "762 [D loss: 0.637113, acc.: 63.71%] [G loss: 1.114128]\n",
            "763 [D loss: 0.676914, acc.: 59.68%] [G loss: 1.147437]\n",
            "764 [D loss: 0.656598, acc.: 63.71%] [G loss: 1.146606]\n",
            "765 [D loss: 0.598143, acc.: 70.97%] [G loss: 1.124380]\n",
            "766 [D loss: 0.660530, acc.: 58.06%] [G loss: 1.089807]\n",
            "767 [D loss: 0.629234, acc.: 62.90%] [G loss: 1.041322]\n",
            "768 [D loss: 0.587663, acc.: 66.13%] [G loss: 1.168397]\n",
            "769 [D loss: 0.700808, acc.: 58.06%] [G loss: 1.006302]\n",
            "770 [D loss: 0.618211, acc.: 64.52%] [G loss: 0.984792]\n",
            "771 [D loss: 0.639951, acc.: 59.68%] [G loss: 1.009198]\n",
            "772 [D loss: 0.519050, acc.: 75.81%] [G loss: 1.183704]\n",
            "773 [D loss: 0.618209, acc.: 67.74%] [G loss: 1.113468]\n",
            "774 [D loss: 0.626127, acc.: 63.71%] [G loss: 1.193384]\n",
            "775 [D loss: 0.619650, acc.: 63.71%] [G loss: 1.024456]\n",
            "776 [D loss: 0.483938, acc.: 77.42%] [G loss: 1.112386]\n",
            "777 [D loss: 0.563571, acc.: 70.97%] [G loss: 1.036330]\n",
            "778 [D loss: 0.738644, acc.: 55.65%] [G loss: 1.102085]\n",
            "779 [D loss: 0.681104, acc.: 63.71%] [G loss: 1.224223]\n",
            "780 [D loss: 0.612912, acc.: 65.32%] [G loss: 0.973333]\n",
            "781 [D loss: 0.525307, acc.: 76.61%] [G loss: 1.084099]\n",
            "782 [D loss: 0.696970, acc.: 58.06%] [G loss: 1.129342]\n",
            "783 [D loss: 0.635325, acc.: 60.48%] [G loss: 0.972553]\n",
            "784 [D loss: 0.735623, acc.: 49.19%] [G loss: 1.036187]\n",
            "785 [D loss: 0.545027, acc.: 71.77%] [G loss: 1.089890]\n",
            "786 [D loss: 0.524254, acc.: 76.61%] [G loss: 0.928497]\n",
            "787 [D loss: 0.598742, acc.: 65.32%] [G loss: 1.225032]\n",
            "788 [D loss: 0.648353, acc.: 61.29%] [G loss: 1.103127]\n",
            "789 [D loss: 0.566163, acc.: 75.81%] [G loss: 1.246286]\n",
            "790 [D loss: 0.705303, acc.: 53.23%] [G loss: 0.998693]\n",
            "791 [D loss: 0.587564, acc.: 66.13%] [G loss: 0.963353]\n",
            "792 [D loss: 0.506897, acc.: 76.61%] [G loss: 1.108606]\n",
            "793 [D loss: 0.720471, acc.: 50.81%] [G loss: 1.020106]\n",
            "794 [D loss: 0.724636, acc.: 54.84%] [G loss: 1.028816]\n",
            "795 [D loss: 0.550533, acc.: 70.97%] [G loss: 1.120263]\n",
            "796 [D loss: 0.734342, acc.: 58.06%] [G loss: 1.128785]\n",
            "797 [D loss: 0.671613, acc.: 60.48%] [G loss: 0.970755]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9okWNZ0dh88K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}